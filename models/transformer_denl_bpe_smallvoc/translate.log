2025-06-02 00:44:28,824 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 00:44:28,902 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 00:44:28,966 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 00:44:28,967 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 00:44:29,007 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 00:44:29,218 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 00:44:29,218 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 00:44:29,225 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 00:44:29,225 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 00:47:23,885 - INFO - joeynmt.prediction - Generation took 174.6400[sec]. (No references given)
2025-06-02 15:55:04,262 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 15:55:04,303 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 15:55:04,369 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 15:55:04,371 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 15:55:04,421 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 15:55:04,638 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 15:55:04,639 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 15:55:04,640 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 15:55:04,640 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:27:16,042 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:27:16,064 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:27:16,186 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:27:16,187 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:27:16,232 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:27:16,416 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:27:16,416 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:27:16,426 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:27:16,426 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:28:02,105 - INFO - joeynmt.prediction - Generation took 45.6445[sec]. (No references given)
2025-06-02 16:28:07,530 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:28:07,551 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:28:07,602 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:28:07,617 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:28:07,658 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:28:07,848 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:28:07,848 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:28:07,850 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:28:07,850 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:29:48,830 - INFO - joeynmt.prediction - Generation took 100.9591[sec]. (No references given)
2025-06-02 16:29:54,210 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:29:54,228 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:29:54,294 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:29:54,294 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:29:54,343 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:29:54,528 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:29:54,528 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:29:54,538 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:29:54,538 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:32:51,705 - INFO - joeynmt.prediction - Generation took 177.1471[sec]. (No references given)
2025-06-02 16:32:56,938 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:32:56,961 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:32:57,024 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:32:57,024 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:32:57,065 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:32:57,260 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:32:57,261 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:32:57,266 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:32:57,266 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:37:00,705 - INFO - joeynmt.prediction - Generation took 243.4106[sec]. (No references given)
2025-06-02 16:37:06,105 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:37:06,127 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:37:06,189 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:37:06,189 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:37:06,239 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:37:06,413 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:37:06,413 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:37:06,421 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:37:06,423 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:42:46,574 - INFO - joeynmt.prediction - Generation took 340.1203[sec]. (No references given)
2025-06-02 16:42:52,031 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:42:52,047 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:42:52,124 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:42:52,125 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:42:52,169 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:42:52,360 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:42:52,360 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:42:52,362 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:42:52,362 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=6, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:49:52,329 - INFO - joeynmt.prediction - Generation took 419.9372[sec]. (No references given)
2025-06-02 16:49:57,898 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:49:57,920 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:49:57,990 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:49:57,991 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:49:58,026 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:49:58,200 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:49:58,200 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:49:58,200 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:49:58,200 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=7, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 16:58:56,354 - INFO - joeynmt.prediction - Generation took 538.1102[sec]. (No references given)
2025-06-02 16:59:13,106 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-02 16:59:13,153 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-02 16:59:13,258 - INFO - joeynmt.model - Enc-dec model built.
2025-06-02 16:59:13,259 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:59:14,004 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-02 16:59:14,170 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:59:14,170 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-02 16:59:14,194 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-02 16:59:14,194 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=8, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-02 17:09:45,513 - INFO - joeynmt.prediction - Generation took 631.2978[sec]. (No references given)
2025-06-03 23:53:08,710 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-03 23:53:08,762 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-03 23:53:09,858 - INFO - joeynmt.model - Enc-dec model built.
2025-06-03 23:53:09,858 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:53:10,688 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:53:11,024 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:53:11,025 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:53:11,047 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-03 23:53:11,047 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-03 23:54:21,903 - INFO - joeynmt.prediction - Generation took 70.8079[sec]. (No references given)
2025-06-03 23:54:28,379 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-03 23:54:28,392 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-03 23:54:28,462 - INFO - joeynmt.model - Enc-dec model built.
2025-06-03 23:54:28,462 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:54:28,508 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:54:28,660 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:54:28,660 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:54:28,660 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-03 23:54:28,660 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-03 23:56:13,213 - INFO - joeynmt.prediction - Generation took 104.5102[sec]. (No references given)
2025-06-03 23:56:19,921 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-03 23:56:19,941 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-03 23:56:20,023 - INFO - joeynmt.model - Enc-dec model built.
2025-06-03 23:56:20,039 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:56:20,094 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:56:20,271 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:56:20,271 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:56:20,271 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-03 23:56:20,271 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-03 23:59:16,567 - INFO - joeynmt.prediction - Generation took 176.2698[sec]. (No references given)
2025-06-03 23:59:22,105 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-03 23:59:22,105 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-03 23:59:22,174 - INFO - joeynmt.model - Enc-dec model built.
2025-06-03 23:59:22,174 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:59:22,227 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-03 23:59:22,370 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:59:22,386 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-03 23:59:22,386 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-03 23:59:22,386 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-04 00:03:27,838 - INFO - joeynmt.prediction - Generation took 245.4292[sec]. (No references given)
2025-06-04 00:03:41,853 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-04 00:03:41,891 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-04 00:03:41,957 - INFO - joeynmt.model - Enc-dec model built.
2025-06-04 00:03:41,957 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:03:42,024 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:03:42,186 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:03:42,186 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:03:42,186 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-04 00:03:42,186 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-04 00:09:30,780 - INFO - joeynmt.prediction - Generation took 348.5627[sec]. (No references given)
2025-06-04 00:09:36,316 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-04 00:09:36,333 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-04 00:09:36,400 - INFO - joeynmt.model - Enc-dec model built.
2025-06-04 00:09:36,400 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:09:36,434 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:09:36,584 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:09:36,584 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:09:36,584 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-04 00:09:36,584 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=6, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-04 00:16:48,516 - INFO - joeynmt.prediction - Generation took 431.8857[sec]. (No references given)
2025-06-04 00:17:02,841 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-04 00:17:02,874 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-04 00:17:02,940 - INFO - joeynmt.model - Enc-dec model built.
2025-06-04 00:17:02,940 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:17:03,007 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:17:03,166 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:17:03,166 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:17:03,172 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-04 00:17:03,172 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=7, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-04 00:26:03,760 - INFO - joeynmt.prediction - Generation took 540.5471[sec]. (No references given)
2025-06-04 00:26:09,512 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-04 00:26:09,528 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-04 00:26:09,594 - INFO - joeynmt.model - Enc-dec model built.
2025-06-04 00:26:09,595 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:26:09,632 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:26:09,789 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:26:09,789 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:26:09,789 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-04 00:26:09,789 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=8, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-04 00:36:49,685 - INFO - joeynmt.prediction - Generation took 639.8640[sec]. (No references given)
2025-06-04 00:36:55,569 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-04 00:36:55,596 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-04 00:36:55,667 - INFO - joeynmt.model - Enc-dec model built.
2025-06-04 00:36:55,668 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:36:55,702 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 00:36:55,847 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:36:55,847 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 00:36:55,847 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-04 00:36:55,863 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=9, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-04 01:08:12,759 - INFO - joeynmt.prediction - Generation took 1876.8670[sec]. (No references given)
2025-06-04 01:08:24,648 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-06-04 01:08:24,686 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-06-04 01:08:24,794 - INFO - joeynmt.model - Enc-dec model built.
2025-06-04 01:08:24,795 - INFO - joeynmt.helpers - Loading model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 01:08:25,501 - INFO - joeynmt.helpers - Load model from D:\CL\mt-exercise-4\models\transformer_denl_bpe_smallvoc\105500.ckpt.
2025-06-04 01:08:25,660 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 01:08:25,660 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-06-04 01:08:25,666 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-06-04 01:08:25,666 - INFO - joeynmt.prediction - Predicting 1779 example(s)... (Beam search with beam_size=10, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-06-04 02:09:10,762 - INFO - joeynmt.prediction - Generation took 3645.0684[sec]. (No references given)
